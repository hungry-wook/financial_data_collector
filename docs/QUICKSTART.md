# Quick Start Guide

## ğŸš€ 5ë¶„ë§Œì— ì‹œì‘í•˜ê¸°

### 1. ì„œë²„ ì‹¤í–‰
```bash
# FastAPI ì„œë²„ ì‹œì‘
cd /c/workspace/financial_data_collector
uv run uvicorn financial_data_collector.server:app --host 0.0.0.0 --port 8000

# ë¸Œë¼ìš°ì €ì—ì„œ ì—´ê¸°:
# http://localhost:8000          - API ì •ë³´
# http://localhost:8000/docs     - Swagger UI (ëŒ€í™”í˜• API ë¬¸ì„œ)
# http://localhost:8000/redoc    - ReDoc (ì½ê¸° ì‰¬ìš´ API ë¬¸ì„œ)
```

### 2. API í…ŒìŠ¤íŠ¸ (Swagger UI ì‚¬ìš©)

1. ë¸Œë¼ìš°ì €ì—ì„œ http://localhost:8000/docs ì—´ê¸°
2. `POST /api/v1/backtest/exports` í´ë¦­
3. "Try it out" ë²„íŠ¼ í´ë¦­
4. Request body ì…ë ¥:
```json
{
  "market_code": "KOSDAQ",
  "index_codes": ["KOSDAQ", "KOSPI"],
  "date_from": "2026-01-02",
  "date_to": "2026-01-03",
  "include_issues": false,
  "output_format": "parquet",
  "output_path": "data/backtest_export_test"
}
```
5. "Execute" í´ë¦­
6. Responseì—ì„œ `job_id` ë³µì‚¬
7. `GET /api/v1/backtest/exports/{job_id}`ë¡œ ìƒíƒœ í™•ì¸

### 3. curlë¡œ í…ŒìŠ¤íŠ¸
```bash
# 1. Export job ìƒì„±
JOB_ID=$(curl -X POST http://localhost:8000/api/v1/backtest/exports \
  -H "Content-Type: application/json" \
  -d '{
    "market_code": "KOSDAQ",
    "index_codes": ["KOSDAQ", "KOSPI"],
    "date_from": "2026-01-02",
    "date_to": "2026-01-03",
    "include_issues": false,
    "output_format": "parquet",
    "output_path": "data/backtest_export_test"
  }' | jq -r '.job_id')

echo "Job ID: $JOB_ID"

# 2. Job ìƒíƒœ í™•ì¸
curl http://localhost:8000/api/v1/backtest/exports/$JOB_ID | jq

# 3. Manifest í™•ì¸ (ì™„ë£Œ í›„)
curl http://localhost:8000/api/v1/backtest/exports/$JOB_ID/manifest | jq
```

### 4. Pythonìœ¼ë¡œ ì‚¬ìš©
```python
import requests
import time

# Export ìš”ì²­
response = requests.post("http://localhost:8000/api/v1/backtest/exports", json={
    "market_code": "KOSDAQ",
    "index_codes": ["KOSDAQ", "KOSPI"],
    "date_from": "2026-01-02",
    "date_to": "2026-01-03",
    "include_issues": False,
    "output_format": "parquet",
    "output_path": "data/my_backtest"
})

job_id = response.json()["job_id"]
print(f"Job created: {job_id}")

# Poll until complete
while True:
    status = requests.get(f"http://localhost:8000/api/v1/backtest/exports/{job_id}").json()

    if status["status"] == "SUCCEEDED":
        print(f"âœ… Export completed!")
        print(f"Files: {status['files']}")
        print(f"Row counts: {status['row_counts']}")
        break
    elif status["status"] == "FAILED":
        print(f"âŒ Export failed: {status.get('error_message')}")
        break
    else:
        print(f"â³ Status: {status['status']}")
        time.sleep(2)

# Read Parquet files
import pyarrow.parquet as pq
df = pq.read_table("data/my_backtest/instrument_daily.parquet").to_pandas()
print(df.head())
```

## ğŸ“Š Daily ë°ì´í„° ìˆ˜ì§‘

### ìˆ˜ë™ ì‹¤í–‰
```bash
# ì˜¤ëŠ˜ ë°ì´í„° ìˆ˜ì§‘
uv run python -m financial_data_collector.collect_krx_data \
  --date-from 2026-01-02 \
  --date-to 2026-01-02

# ë””ë²„ê·¸ ëª¨ë“œ
uv run python -m financial_data_collector.collect_krx_data \
  --date-from 2026-01-02 \
  --date-to 2026-01-02 \
  --debug
```

### Cron ì„¤ì • (Linux/Mac)
```bash
# crontab -e
0 18 * * * cd /app && uv run python -m financial_data_collector.collect_krx_data --date-from $(date -d "yesterday" +\%Y-\%m-\%d) --date-to $(date -d "yesterday" +\%Y-\%m-\%d) >> /var/log/krx.log 2>&1
```

### Windows ì‘ì—… ìŠ¤ì¼€ì¤„ëŸ¬
```powershell
# collect_daily.ps1
$yesterday = (Get-Date).AddDays(-1).ToString("yyyy-MM-dd")
cd C:\workspace\financial_data_collector
uv run python -m financial_data_collector.collect_krx_data --date-from $yesterday --date-to $yesterday
```

## ğŸ¯ ìš´ì˜ ì‹œë‚˜ë¦¬ì˜¤

### ì‹œë‚˜ë¦¬ì˜¤ 1: ë°±í…ŒìŠ¤íŠ¸ ì¤€ë¹„ (ê°€ì¥ ì¼ë°˜ì )

```bash
# 1. ë°ì´í„° ìˆ˜ì§‘ (í•œ ë²ˆë§Œ)
uv run python -m financial_data_collector.collect_krx_data \
  --date-from 2024-01-01 \
  --date-to 2024-12-31

# 2. ì„œë²„ ì‹¤í–‰ (ìƒì‹œ)
uv run uvicorn financial_data_collector.server:app --host 0.0.0.0 --port 8000

# 3. Parquet export (í•„ìš”í•  ë•Œë§ˆë‹¤)
curl -X POST http://localhost:8000/api/v1/backtest/exports \
  -H "Content-Type: application/json" \
  -d '{
    "market_code": "KOSDAQ",
    "index_codes": ["KOSDAQ", "KOSPI"],
    "date_from": "2024-01-01",
    "date_to": "2024-12-31",
    "output_path": "data/backtest_2024"
  }'
```

### ì‹œë‚˜ë¦¬ì˜¤ 2: Daily ìš´ì˜

```bash
# Option A: ë‘ ê°œì˜ í”„ë¡œì„¸ìŠ¤
# Terminal 1: Daily ìˆ˜ì§‘ (Cron)
# Terminal 2: API ì„œë²„
uv run uvicorn financial_data_collector.server:app

# Option B: Docker Compose (ê¶Œì¥)
docker-compose up -d

# Option C: Systemd (Linux í”„ë¡œë•ì…˜)
sudo systemctl start krx-collector  # Cron
sudo systemctl start krx-api        # API
```

### ì‹œë‚˜ë¦¬ì˜¤ 3: ëŒ€ëŸ‰ ë°±í…ŒìŠ¤íŠ¸

```python
# ì—¬ëŸ¬ ê¸°ê°„ì˜ ë°ì´í„°ë¥¼ ë³‘ë ¬ë¡œ export
import requests
from concurrent.futures import ThreadPoolExecutor

periods = [
    ("2024-01-01", "2024-03-31", "q1"),
    ("2024-04-01", "2024-06-30", "q2"),
    ("2024-07-01", "2024-09-30", "q3"),
    ("2024-10-01", "2024-12-31", "q4"),
]

def export_period(date_from, date_to, label):
    response = requests.post("http://localhost:8000/api/v1/backtest/exports", json={
        "market_code": "KOSDAQ",
        "index_codes": ["KOSDAQ", "KOSPI"],
        "date_from": date_from,
        "date_to": date_to,
        "output_path": f"data/backtest_2024_{label}"
    })
    return response.json()["job_id"]

# ë³‘ë ¬ ì‹¤í–‰
with ThreadPoolExecutor(max_workers=4) as executor:
    jobs = list(executor.map(lambda p: export_period(*p), periods))
    print(f"Created {len(jobs)} export jobs")
```

## ğŸ” ë°ì´í„° í™•ì¸

### SQLiteë¡œ í™•ì¸
```bash
sqlite3 data/financial_data.db

# ìˆ˜ì§‘ëœ ë°ì´í„° ìš”ì•½
SELECT
    COUNT(DISTINCT instrument_id) as instruments,
    COUNT(*) as daily_records,
    MIN(trade_date) as from_date,
    MAX(trade_date) as to_date
FROM daily_market_data;

# ìµœê·¼ ìˆ˜ì§‘ ìƒíƒœ
SELECT * FROM collection_runs ORDER BY started_at DESC LIMIT 5;
```

### Parquet íŒŒì¼ í™•ì¸
```python
import pyarrow.parquet as pq
import pandas as pd

# ë©”íƒ€ë°ì´í„° í™•ì¸
metadata = pq.read_metadata("data/backtest_export/instrument_daily.parquet")
print(f"Rows: {metadata.num_rows}")
print(f"Columns: {metadata.num_columns}")

# ë°ì´í„° ë¡œë“œ
df = pq.read_table("data/backtest_export/instrument_daily.parquet").to_pandas()
print(df.info())
print(df.describe())

# ìƒ˜í”Œ ë°ì´í„°
print(df.head())
```

## ğŸ†˜ ë¬¸ì œ í•´ê²°

### ì„œë²„ê°€ ì‹œì‘ë˜ì§€ ì•ŠìŒ
```bash
# Port 8000ì´ ì´ë¯¸ ì‚¬ìš© ì¤‘ì¸ì§€ í™•ì¸
lsof -i :8000  # Linux/Mac
netstat -ano | findstr :8000  # Windows

# ë‹¤ë¥¸ í¬íŠ¸ ì‚¬ìš©
uv run uvicorn financial_data_collector.server:app --port 8001
```

### Exportê°€ ì‹¤íŒ¨í•¨
```bash
# ë””ìŠ¤í¬ ê³µê°„ í™•ì¸
df -h  # Linux/Mac
wmic logicaldisk get size,freespace,caption  # Windows

# Output ê²½ë¡œ ê¶Œí•œ í™•ì¸
ls -la data/

# ë¡œê·¸ í™•ì¸
# ì„œë²„ ë¡œê·¸ì—ì„œ ìƒì„¸ ì—ëŸ¬ ë©”ì‹œì§€ í™•ì¸
```

### ë°ì´í„°ê°€ ì—†ìŒ
```bash
# DB íŒŒì¼ í™•ì¸
ls -lh data/financial_data.db

# ë°ì´í„° í™•ì¸
sqlite3 data/financial_data.db "SELECT COUNT(*) FROM daily_market_data;"

# ë°ì´í„° ìˆ˜ì§‘ ë‹¤ì‹œ ì‹¤í–‰
uv run python -m financial_data_collector.collect_krx_data \
  --date-from 2026-01-02 --date-to 2026-01-03
```

## ğŸ“š ë” ìì„¸í•œ ë‚´ìš©

- [BULK_EXPORT_API_SPEC.md](./BULK_EXPORT_API_SPEC.md) - API ìƒì„¸ ìŠ¤í™
- [DEPLOYMENT_GUIDE.md](./DEPLOYMENT_GUIDE.md) - í”„ë¡œë•ì…˜ ë°°í¬ ê°€ì´ë“œ
- [README.md](../README.md) - í”„ë¡œì íŠ¸ ê°œìš”

## ğŸ’¡ Tips

1. **Swagger UI í™œìš©**: http://localhost:8000/docs ì—ì„œ ëª¨ë“  APIë¥¼ ì§ì ‘ í…ŒìŠ¤íŠ¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
2. **ë””ë²„ê·¸ ëª¨ë“œ**: `--debug` í”Œë˜ê·¸ë¡œ ìƒì„¸ ë¡œê·¸ í™•ì¸
3. **ì‘ì€ ê¸°ê°„ë¶€í„°**: ì²˜ìŒì—ëŠ” 1-2ì¼ì¹˜ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸ í›„ í™•ì¥
4. **ë””ìŠ¤í¬ ê³µê°„**: 1ë…„ì¹˜ ë°ì´í„° ~10GB ì •ë„ í•„ìš” (Parquet í¬í•¨)
5. **ë°±ì—…**: SQLite DB íŒŒì¼ì„ ì •ê¸°ì ìœ¼ë¡œ ë°±ì—…
