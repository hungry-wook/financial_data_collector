# KRX Data Collector - ìš´ì˜ ê°€ì´ë“œ

## ğŸ“‹ ëª©ì°¨
1. [ì•„í‚¤í…ì²˜ ê°œìš”](#ì•„í‚¤í…ì²˜-ê°œìš”)
2. [Daily ë°ì´í„° ìˆ˜ì§‘](#daily-ë°ì´í„°-ìˆ˜ì§‘)
3. [FastAPI ì„œë²„ ìš´ì˜](#fastapi-ì„œë²„-ìš´ì˜)
4. [ëª¨ë‹ˆí„°ë§ ë° ìœ ì§€ë³´ìˆ˜](#ëª¨ë‹ˆí„°ë§-ë°-ìœ ì§€ë³´ìˆ˜)

## ğŸ—ï¸ ì•„í‚¤í…ì²˜ ê°œìš”

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Cron Job       â”‚  ë§¤ì¼ 18:00 - ì „ë‚  ë°ì´í„° ìˆ˜ì§‘
â”‚  (Daily ìˆ˜ì§‘)   â”‚  â†’ SQLite DB ì €ì¥
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SQLite DB      â”‚  ëª¨ë“  ìˆ˜ì§‘ ë°ì´í„° ì €ì¥
â”‚  (Local)        â”‚  - instruments
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  - daily_market_data
         â”‚           - benchmark_index_data
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FastAPI Server â”‚  Backtest Export API
â”‚  (Port 8000)    â”‚  â†’ Parquet íŒŒì¼ ìƒì„±
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“Š Daily ë°ì´í„° ìˆ˜ì§‘

### 1. ìˆ˜ë™ ì‹¤í–‰
```bash
# ì–´ì œ ë°ì´í„° ìˆ˜ì§‘
uv run python -m financial_data_collector.collect_krx_data \
  --date-from $(date -d "yesterday" +%Y-%m-%d) \
  --date-to $(date -d "yesterday" +%Y-%m-%d)

# íŠ¹ì • ê¸°ê°„ ìˆ˜ì§‘
uv run python -m financial_data_collector.collect_krx_data \
  --date-from 2026-01-02 \
  --date-to 2026-01-10
```

### 2. Cron Job ì„¤ì • (Linux)
```bash
# crontab í¸ì§‘
crontab -e

# ë§¤ì¼ ì˜¤í›„ 6ì‹œì— ì‹¤í–‰
0 18 * * * cd /app/financial_data_collector && \
  uv run python -m financial_data_collector.collect_krx_data \
  --date-from $(date -d "yesterday" +\%Y-\%m-\%d) \
  --date-to $(date -d "yesterday" +\%Y-\%m-\%d) \
  >> /var/log/krx_collection.log 2>&1
```

### 3. Windows ì‘ì—… ìŠ¤ì¼€ì¤„ëŸ¬
```powershell
# PowerShell ìŠ¤í¬ë¦½íŠ¸ ìƒì„±: collect_daily.ps1
$yesterday = (Get-Date).AddDays(-1).ToString("yyyy-MM-dd")
cd C:\workspace\financial_data_collector
uv run python -m financial_data_collector.collect_krx_data --date-from $yesterday --date-to $yesterday

# ì‘ì—… ìŠ¤ì¼€ì¤„ëŸ¬ì— ë“±ë¡
# - íŠ¸ë¦¬ê±°: ë§¤ì¼ ì˜¤í›„ 6ì‹œ
# - ì‘ì—…: powershell.exe -File "C:\path\to\collect_daily.ps1"
```

### 4. Docker Cron (ê¶Œì¥)
```dockerfile
# Dockerfile
FROM python:3.12-slim

WORKDIR /app
COPY . .
RUN pip install uv && uv sync

# Cron ì„¤ì •
RUN apt-get update && apt-get install -y cron
COPY crontab /etc/cron.d/krx-collector
RUN chmod 0644 /etc/cron.d/krx-collector
RUN crontab /etc/cron.d/krx-collector

CMD ["cron", "-f"]
```

```bash
# crontab íŒŒì¼
0 18 * * * cd /app && uv run python -m financial_data_collector.collect_krx_data --date-from $(date -d "yesterday" +\%Y-\%m-\%d) --date-to $(date -d "yesterday" +\%Y-\%m-\%d) >> /var/log/cron.log 2>&1
```

## ğŸš€ FastAPI ì„œë²„ ìš´ì˜

### 1. ë¡œì»¬ ê°œë°œ ì‹¤í–‰
```bash
# ì§ì ‘ ì‹¤í–‰
uv run python -m financial_data_collector.server

# ë˜ëŠ” uvicorn ì‚¬ìš©
uv run uvicorn financial_data_collector.server:app --reload --host 0.0.0.0 --port 8000
```

### 2. Production ì‹¤í–‰ (Gunicorn + Uvicorn Workers)
```bash
# ì„¤ì¹˜
uv add gunicorn

# ì‹¤í–‰ (4 workers)
uv run gunicorn financial_data_collector.server:app \
  --workers 4 \
  --worker-class uvicorn.workers.UvicornWorker \
  --bind 0.0.0.0:8000 \
  --timeout 300 \
  --access-logfile /var/log/api_access.log \
  --error-logfile /var/log/api_error.log
```

### 3. Systemd Service (Linux)
```ini
# /etc/systemd/system/krx-api.service
[Unit]
Description=KRX Backtest Export API
After=network.target

[Service]
Type=notify
User=app
Group=app
WorkingDirectory=/app/financial_data_collector
Environment="PATH=/app/financial_data_collector/.venv/bin"
ExecStart=/app/financial_data_collector/.venv/bin/gunicorn \
  financial_data_collector.server:app \
  --workers 4 \
  --worker-class uvicorn.workers.UvicornWorker \
  --bind 0.0.0.0:8000 \
  --timeout 300
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target
```

```bash
# ì„œë¹„ìŠ¤ ì‹œì‘
sudo systemctl daemon-reload
sudo systemctl enable krx-api
sudo systemctl start krx-api
sudo systemctl status krx-api

# ë¡œê·¸ í™•ì¸
journalctl -u krx-api -f
```

### 4. Docker Compose (ê¶Œì¥)
```yaml
# docker-compose.yml
version: '3.8'

services:
  # Data Collector (Cron)
  collector:
    build: .
    volumes:
      - ./data:/app/data
      - ./logs:/var/log
    environment:
      - KRX_AUTH_KEY=${KRX_AUTH_KEY}
      - KRX_DAILY_LIMIT=10000
    restart: unless-stopped

  # API Server
  api:
    build: .
    command: >
      gunicorn financial_data_collector.server:app
      --workers 4
      --worker-class uvicorn.workers.UvicornWorker
      --bind 0.0.0.0:8000
      --timeout 300
    ports:
      - "8000:8000"
    volumes:
      - ./data:/app/data
    environment:
      - DB_PATH=/app/data/financial_data.db
    restart: unless-stopped
    depends_on:
      - collector

  # Nginx (Optional - for production)
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
      - ./ssl:/etc/nginx/ssl:ro
    depends_on:
      - api
    restart: unless-stopped
```

```bash
# ì‹¤í–‰
docker-compose up -d

# ë¡œê·¸ í™•ì¸
docker-compose logs -f api

# ì¢…ë£Œ
docker-compose down
```

## ğŸ”§ API ì‚¬ìš© ì˜ˆì‹œ

### 1. Export Job ìƒì„±
```bash
curl -X POST http://localhost:8000/api/v1/backtest/exports \
  -H "Content-Type: application/json" \
  -d '{
    "market_code": "KOSDAQ",
    "index_codes": ["KOSDAQ", "KOSPI"],
    "date_from": "2026-01-02",
    "date_to": "2026-01-10",
    "include_issues": true,
    "output_format": "parquet",
    "output_path": "/data/exports/2026_q1"
  }'

# Response:
# {
#   "job_id": "abc-123-def",
#   "status": "PENDING",
#   "submitted_at": "2026-02-20T10:00:00Z"
# }
```

### 2. Job ìƒíƒœ í™•ì¸
```bash
curl http://localhost:8000/api/v1/backtest/exports/abc-123-def

# Response (Running):
# {
#   "job_id": "abc-123-def",
#   "status": "RUNNING",
#   "progress": 60,
#   "started_at": "2026-02-20T10:00:05Z"
# }

# Response (Completed):
# {
#   "job_id": "abc-123-def",
#   "status": "SUCCEEDED",
#   "output_path": "/data/exports/2026_q1",
#   "files": [...],
#   "row_counts": {...}
# }
```

### 3. Manifest í™•ì¸
```bash
curl http://localhost:8000/api/v1/backtest/exports/abc-123-def/manifest

# Response:
# {
#   "job_id": "abc-123-def",
#   "market_code": "KOSDAQ",
#   "date_from": "2026-01-02",
#   "date_to": "2026-01-10",
#   "files": [
#     {
#       "name": "instrument_daily.parquet",
#       "rows": 16136,
#       "sha256": "..."
#     },
#     ...
#   ]
# }
```

## ğŸ“Š ëª¨ë‹ˆí„°ë§ ë° ìœ ì§€ë³´ìˆ˜

### 1. ë°ì´í„° ìˆ˜ì§‘ ëª¨ë‹ˆí„°ë§
```bash
# ìµœê·¼ ìˆ˜ì§‘ Run í™•ì¸
sqlite3 data/financial_data.db "
SELECT run_id, status, success_count, failure_count,
       started_at, finished_at
FROM collection_runs
ORDER BY started_at DESC
LIMIT 10;
"

# ë°ì´í„° í’ˆì§ˆ ì´ìŠˆ í™•ì¸
sqlite3 data/financial_data.db "
SELECT issue_code, severity, COUNT(*) as count
FROM data_quality_issues
GROUP BY issue_code, severity
ORDER BY count DESC;
"
```

### 2. ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
```bash
# DB í¬ê¸° í™•ì¸
du -h data/financial_data.db

# Export í´ë” í¬ê¸° í™•ì¸
du -sh data/backtest_export/

# ì˜¤ë˜ëœ export ì‚­ì œ (30ì¼ ì´ìƒ)
find data/backtest_export/ -type d -mtime +30 -exec rm -rf {} \;
```

### 3. API í—¬ìŠ¤ì²´í¬
```bash
# Health endpoint
curl http://localhost:8000/health

# Prometheus metrics (ì¶”ê°€ êµ¬í˜„ í•„ìš”)
# curl http://localhost:8000/metrics
```

### 4. ë¡œê·¸ ê´€ë¦¬
```bash
# API ë¡œê·¸ í™•ì¸
tail -f /var/log/api_access.log
tail -f /var/log/api_error.log

# Collection ë¡œê·¸ í™•ì¸
tail -f /var/log/krx_collection.log

# ë¡œê·¸ ë¡œí…Œì´ì…˜ ì„¤ì • (/etc/logrotate.d/krx-api)
/var/log/api_*.log {
    daily
    rotate 30
    compress
    delaycompress
    notifempty
    create 0640 app app
    sharedscripts
    postrotate
        systemctl reload krx-api > /dev/null 2>&1 || true
    endscript
}
```

## ğŸ”’ ë³´ì•ˆ ê³ ë ¤ì‚¬í•­

### 1. API ì¸ì¦ (ì¶”ì²œ ì¶”ê°€ êµ¬í˜„)
```python
# server.pyì— ì¶”ê°€
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials

security = HTTPBearer()

@app.post("/api/v1/backtest/exports", dependencies=[Depends(verify_token)])
async def create_export(...):
    ...
```

### 2. Rate Limiting
```python
# slowapi ì‚¬ìš©
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address

limiter = Limiter(key_func=get_remote_address)
app.state.limiter = limiter

@app.post("/api/v1/backtest/exports")
@limiter.limit("10/minute")
async def create_export(...):
    ...
```

### 3. CORS ì„¤ì • (í•„ìš”ì‹œ)
```python
from fastapi.middleware.cors import CORSMiddleware

app.add_middleware(
    CORSMiddleware,
    allow_origins=["https://yourdomain.com"],
    allow_methods=["GET", "POST"],
    allow_headers=["*"],
)
```

## ğŸ“ˆ ì„±ëŠ¥ ìµœì í™”

### 1. DB Index ì¶”ê°€
```sql
-- ìì£¼ ì¡°íšŒë˜ëŠ” ì»¬ëŸ¼ì— ì¸ë±ìŠ¤
CREATE INDEX idx_daily_trade_date ON daily_market_data(trade_date);
CREATE INDEX idx_daily_instrument ON daily_market_data(instrument_id);
CREATE INDEX idx_benchmark_date ON benchmark_index_data(trade_date);
```

### 2. Background Job Queue (ëŒ€ëŸ‰ Exportìš©)
```bash
# Redis + Celery ì‚¬ìš© ê¶Œì¥
uv add celery[redis]

# celery worker ì‹¤í–‰
celery -A financial_data_collector.tasks worker --loglevel=info
```

### 3. Cache Layer (ì„ íƒì )
```python
# Redis cache for manifest
from fastapi_cache import FastAPICache
from fastapi_cache.backends.redis import RedisBackend

@app.on_event("startup")
async def startup():
    redis = aioredis.from_url("redis://localhost")
    FastAPICache.init(RedisBackend(redis), prefix="krx-cache")
```

## ğŸ†˜ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ë¬¸ì œ: ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨
```bash
# 1. API Key í™•ì¸
echo $KRX_AUTH_KEY

# 2. API í˜¸ì¶œ ì œí•œ í™•ì¸
sqlite3 data/financial_data.db "
SELECT COUNT(*) as api_calls
FROM collection_runs
WHERE DATE(started_at) = DATE('now');
"

# 3. ë¡œê·¸ í™•ì¸
grep -i error /var/log/krx_collection.log
```

### ë¬¸ì œ: Export ì‹¤íŒ¨
```bash
# ë””ìŠ¤í¬ ê³µê°„ í™•ì¸
df -h

# Permission í™•ì¸
ls -la data/backtest_export/

# DB lock í™•ì¸
lsof | grep financial_data.db
```

## ğŸ“ Checklist

**ìš´ì˜ ì‹œì‘ ì „ í™•ì¸ì‚¬í•­:**
- [ ] KRX API Key ì„¤ì • (.env íŒŒì¼)
- [ ] Cron job ì„¤ì • ë° í…ŒìŠ¤íŠ¸
- [ ] FastAPI ì„œë²„ ì •ìƒ ì‹¤í–‰ í™•ì¸
- [ ] ë””ìŠ¤í¬ ê³µê°„ ì¶©ë¶„í•œì§€ í™•ì¸ (ìµœì†Œ 100GB ê¶Œì¥)
- [ ] ë¡œê·¸ ë¡œí…Œì´ì…˜ ì„¤ì •
- [ ] ë°±ì—… ì „ëµ ìˆ˜ë¦½ (DB ë°±ì—…)
- [ ] ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ êµ¬ì¶• (Grafana + Prometheus)
- [ ] ì•Œë¦¼ ì„¤ì • (ìˆ˜ì§‘ ì‹¤íŒ¨ ì‹œ Slack/Email)

**ì •ê¸° ì ê²€ (ì£¼ê°„):**
- [ ] ë°ì´í„° ìˆ˜ì§‘ ì„±ê³µë¥  í™•ì¸
- [ ] ë°ì´í„° í’ˆì§ˆ ì´ìŠˆ ë¦¬ë·°
- [ ] ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰ í™•ì¸
- [ ] ë¡œê·¸ ê²€í† 

**ì •ê¸° ì ê²€ (ì›”ê°„):**
- [ ] DB ë°±ì—… ë° ë³µêµ¬ í…ŒìŠ¤íŠ¸
- [ ] ì˜¤ë˜ëœ export íŒŒì¼ ì •ë¦¬
- [ ] ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë¦¬ë·°
- [ ] ë³´ì•ˆ íŒ¨ì¹˜ ì ìš©
